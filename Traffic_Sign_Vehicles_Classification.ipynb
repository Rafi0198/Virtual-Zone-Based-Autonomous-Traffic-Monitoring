{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Necessary Libraries (Database Training Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!nvidia-smi\n",
    "pip install wandb\n",
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Training Configuration (Cloud Environment: Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build from YAML and transfer weights\n",
    "model = YOLO('yolov8x.yaml').load('yolov8x.pt')  \n",
    "\n",
    "# Training The Final Model\n",
    "results = model.train(data=\"/kaggle/input/addbdvss/traffic_update.yaml\",epochs=30, imgsz = 416, batch = 64 ,lr0=0.0001, dropout= 0.15, device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "!yolo task=detect mode=train model=yolov8x.pt data=\"/kaggle/input/addbdvss/traffic_update.yaml\" epochs=30 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Up Occupied GPU Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Necessary Libraries (Test Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install ultralytics\n",
    "!pip install cvzone\n",
    "!pip install supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection and Classification of Traffic Signs and Vehicles (Cloud Environment: Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from cvzone import putTextRect\n",
    "import numpy as np\n",
    "from cvzone import cornerRect\n",
    "import supervision as sv\n",
    "\n",
    "# Define the directory to store videos\n",
    "VIDEOS_DIR = os.path.join('.', 'videos')\n",
    "\n",
    "# Specify the input video file and output video file paths\n",
    "video_path = os.path.join(VIDEOS_DIR, r\"/content/drive/MyDrive/YoLo_Thesis/BDTS/Sample Inputs/1131.mp4\")\n",
    "video_path_out = '{}_out.mp4'.format(video_path)\n",
    "\n",
    "# Open the input video file for reading\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read the first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Get the height, width, and number of channels of the frame\n",
    "H, W, _ = frame.shape\n",
    "\n",
    "# Coordinates of the rectangular portion (ROI)\n",
    "roi_x1, roi_y1, roi_x2, roi_y2 = 0, int(H/7), W, int((3*H)/5)  # Update these coordinates according to your ROI\n",
    "\n",
    "# Create a VideoWriter object to save the output video\n",
    "out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*'MP4V'), int(cap.get(cv2.CAP_PROP_FPS)), (W, H))\n",
    "\n",
    "# Specify the path of the YOLO model\n",
    "model_path1 = os.path.join('.', r\"/content/drive/MyDrive/YoLo_Thesis/BDTS/runskg2/best(1).pt\")\n",
    "model_path2 = os.path.join('.', r\"/content/drive/MyDrive/YoLo_Thesis/BDV/best.pt\")\n",
    "\n",
    "# Load the YOLO model\n",
    "model1 = YOLO(model_path1)\n",
    "model2 = YOLO(model_path2)\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "trace_annotator = sv.TraceAnnotator()\n",
    "\n",
    "\n",
    "# Set a threshold for object detection confidence\n",
    "threshold = 0.5\n",
    "\n",
    "Ap = np.array([14,15,22,23,24,25,26])\n",
    "Ad = np.array([1,2,11,12,13,16,21])\n",
    "Am= np.array([4,5,6,17,18,19])\n",
    "\n",
    "\n",
    "# Process each frame of the input video\n",
    "while ret:\n",
    "\n",
    "    # Extract the ROI from the frame\n",
    "    roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "    active_zone = False\n",
    "\n",
    "    # Perform object detection on the ROI using the YOLO model\n",
    "    results1 = model1(roi)[0]\n",
    "    results2 = model2(frame)[0]\n",
    "\n",
    "    # Iterate through the detected objects in the ROI\n",
    "    for result1 in results1.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result1\n",
    "\n",
    "        # Check if the detection confidence is above the threshold\n",
    "        if score > threshold:\n",
    "            # Adjust the coordinates to the original frame\n",
    "            x1 += roi_x1\n",
    "            x2 += roi_x1\n",
    "            y1 += roi_y1\n",
    "            y2 += roi_y1\n",
    "\n",
    "            class_name = results1.names[int(class_id)].upper()\n",
    "            confidence = round(score * 100, 2)\n",
    "\n",
    "            #cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 4)\n",
    "            cornerRect(frame, (int(x1), int(y1), int(x2)-int(x1), int(y2)-int(y1)), l=6, rt=4, t=3, colorR=(0, 0, 255), colorC=(0, 255, 0))\n",
    "            #cv2.putText(frame, f'{class_name} {confidence}%', (int(x1), int(y1 - 10)),\n",
    "                        #cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "            putTextRect(frame, f'{class_name} {confidence}%', [int(x1), int(y1) -15],\n",
    "                        scale=1.5, thickness=2,colorR=(0,0,255), offset=7)\n",
    "\n",
    "           #putTextRect(frame, f'{class_name} {confidence}%', [int(x1), int(y1) - 20],\n",
    "                        #scale=1.5, thickness=2)\n",
    "\n",
    "            if class_id in Ap:\n",
    "\n",
    "             putTextRect(frame, f'{\"PROHIBITORY\"}', [int(x1) + 8, int(y2) +30],\n",
    "                        scale=1.5, thickness=2,colorR=(0,0,255))\n",
    "\n",
    "\n",
    "            elif class_id in Ad:\n",
    "\n",
    "             putTextRect(frame, f'{\"DANGER\"}', [int(x1) + 8, int(y2) +30],\n",
    "                        scale=1.5, thickness=2,colorR=(0,0,255))\n",
    "\n",
    "\n",
    "            elif class_id in Am:\n",
    "\n",
    "             putTextRect(frame, f'{\"MANDATORY\"}', [int(x1) + 8, int(y2) +30],\n",
    "                        scale=1.5, thickness=2,colorR=(255,0,0))\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "             putTextRect(frame, f'{\"OTHERS\"}', [int(x1) + 8, int(y2) +30],\n",
    "                        scale=1.5, thickness=2,colorR=(0,128,255))\n",
    "\n",
    "            # Check if the current bounding box is within the specified ROI\n",
    "            if ((x1 >= roi_x1) and (x2 <= roi_x2) and (y1 >= roi_y1) and (y2 <= roi_y2)):\n",
    "                    active_zone = True\n",
    "\n",
    "    # Check the condition here based on the active_zone variable\n",
    "\n",
    "    if active_zone:\n",
    "             cv2.rectangle(frame, (0, int(H/7)), (W, int((3*H)/5)), (255, 0, 0), 4)\n",
    "             #cv2.putText(frame, \"Active Virtual Zone\", (int(W / 2) - 150, int(H / 2)), cv2.FONT_HERSHEY_COMPLEX, 1.3, (255, 0, 0), 3)\n",
    "             putTextRect(frame, f'{\"Active Virtual Zone\"}', [int(W / 2) - 170, int(3*H /5)+10],\n",
    "                        scale=2.5, thickness=2,colorR=(255,0,0))\n",
    "\n",
    "    else:\n",
    "             cv2.rectangle(frame, (0, int(H/7)), (W, int((3*H)/5)), (0, 0, 255), 4)\n",
    "             #cv2.putText(frame, \"Inactive Virtual Zone\", (int(W / 2) - 150, int(H / 2)), cv2.FONT_HERSHEY_COMPLEX, 1.3, (0, 255, 255), 3)\n",
    "             putTextRect(frame, f'{\"Inactive Virtual Zone\"}', [int(W / 2) - 170, int(3*H /5)+10],\n",
    "                        scale=2.5, thickness=2,colorR=(0,0,255))\n",
    "\n",
    "    # for result2 in results2.boxes.data.tolist():\n",
    "    #     x1, y1, x2, y2, score, class_id = result2\n",
    "\n",
    "    #     if score > threshold:\n",
    "\n",
    "    #         class_name = results2.names[int(class_id)].upper()\n",
    "    #         confidence = round(score * 100, 2)\n",
    "    #         #cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 4)\n",
    "    #         cornerRect(frame, (int(x1), int(y1), int(x2)-int(x1), int(y2)-int(y1)), l=8, rt=2, t=5, colorR=(153, 51, 255), colorC=(0, 255, 0))\n",
    "    #         #cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),\n",
    "    #                     #cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    #         putTextRect(frame, f'{class_name} {confidence}%', [int(x1), int(y1) -10],\n",
    "    #                             scale=1, thickness=2, colorR=(153, 51, 255), offset=5)\n",
    "\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results2)\n",
    "    detections = detections.confidence > 0.5\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "    labels = [\n",
    "          #model2.model.names[class_id]\n",
    "          #for class_id\n",
    "          #in detections.class_id\n",
    "\n",
    "        f\"{tracker_id} {results2.names[class_id].upper()} {(confidence*100):.1f}%\"\n",
    "        for class_id, tracker_id, confidence\n",
    "        in zip(detections.class_id, detections.tracker_id, detections.confidence)\n",
    "      ]\n",
    "\n",
    "    annotated_image = bounding_box_annotator.annotate(\n",
    "          scene=frame, detections=detections)\n",
    "    annotated_image = label_annotator.annotate(\n",
    "          scene=annotated_image, detections=detections, labels=labels)\n",
    "    annotated_image = trace_annotator.annotate(\n",
    "          scene=annotated_image, detections=detections)\n",
    "\n",
    "\n",
    "    # Write the frame to the output video file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
