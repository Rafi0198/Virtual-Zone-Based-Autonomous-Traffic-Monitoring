{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Necessary Libraries (Database Training Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!nvidia-smi\n",
    "pip install wandb\n",
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Training Configuration (Cloud Environment: Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build from YAML and transfer weights\n",
    "model = YOLO('yolov8x.yaml').load('yolov8x.pt')  \n",
    "\n",
    "# Training The Final Model\n",
    "results = model.train(data=\"/kaggle/input/addbdvss/traffic_update.yaml\",epochs=30, imgsz = 416, batch = 64 ,lr0=0.0001, dropout= 0.15, device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "!yolo task=detect mode=train model=yolov8x.pt data=\"/kaggle/input/addbdvss/traffic_update.yaml\" epochs=30 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Up Occupied GPU Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Necessary Libraries (Test Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffrom google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install ultralytics\n",
    "!pip install cvzone\n",
    "!pip install supervision\n",
    "!pip install easyocr\n",
    "!pip install PIL\n",
    "!pip install --upgrade git+https://github.com/kbatsuren/wiktra/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognition of Vehicle's Registration Plate  (Cloud Environment: Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import necessary libraries\n",
    "    import os\n",
    "    from ultralytics import YOLO\n",
    "    import cv2\n",
    "    from cvzone import putTextRect\n",
    "    import numpy as np\n",
    "    from cvzone import cornerRect\n",
    "    import supervision as sv\n",
    "    import easyocr\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    #from wiktra.Wiktra import Transliterator\n",
    "    from wiktra.Wiktra import translite as tr\n",
    "    import unicodedata\n",
    "    import re\n",
    "\n",
    "    def extract_number(input_string):\n",
    "      # Use regex to find all sequences of digits in the string\n",
    "      numbers = re.findall(r'\\d+', input_string)\n",
    "\n",
    "      # Join all the found numbers into a single string\n",
    "      number_string = ''.join(numbers)\n",
    "\n",
    "      # Keep only the last 6 digits\n",
    "      last_six_digits = number_string[-6:]\n",
    "\n",
    "      # Separate the first two digits from the last four digits with a hyphen\n",
    "      formatted_number = f\"{last_six_digits[:2]}-{last_six_digits[2:]}\"\n",
    "\n",
    "      return formatted_number\n",
    "\n",
    "\n",
    "    image_path = r\"/content/drive/MyDrive/YoLo_Thesis/BDTS/Sample Inputs/1191.jpg\"\n",
    "\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "\n",
    "    # Get the height, width, and number of channels of the frame\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    # Coordinates of the rectangular portion (ROI)\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = 0, int(H/5), W, int((4.5*H)/5)  # Update these coordinates according to your ROI 3.3H/5\n",
    "    cv2.rectangle(frame,(roi_x1, roi_y1),(roi_x2, roi_y2), (0,0,255), 4)\n",
    "\n",
    "\n",
    "    # Specify the path of the YOLO model\n",
    "\n",
    "   # model_path2 = os.path.join('.', r\"/content/drive/MyDrive/YoLo_Thesis/BDV/best.pt\")\n",
    "    model_path3 = os.path.join('.', r\"/content/drive/MyDrive/YoLo_Thesis/LPD4k/best.pt\")\n",
    "\n",
    "\n",
    "    # Load the YOLO model\n",
    "    #model2 = YOLO(model_path2)\n",
    "    model3 = YOLO(model_path3)\n",
    "\n",
    "    tracker = sv.ByteTrack()\n",
    "    trace_annotator = sv.TraceAnnotator()\n",
    "\n",
    "    #Initialize the easyocr model\n",
    "    reader = easyocr.Reader(['bn'])\n",
    "\n",
    "\n",
    "    # Set a threshold for object detection confidence\n",
    "    threshold = 0.5\n",
    "\n",
    "\n",
    "    # Extract the ROI from the frame\n",
    "    roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Perform object detection on the ROI using the YOLO model\n",
    "\n",
    "    results3  = model3(roi)[0]\n",
    "    #print(f\"Inference Time: {results3.speed.get('inference')}ms\")\n",
    "\n",
    "\n",
    "    def add_bangla_text(image, text, position, font_path, font_size, color):\n",
    "      # Convert the OpenCV image to a PIL image\n",
    "      image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "      draw = ImageDraw.Draw(image_pil)\n",
    "\n",
    "      # Load the font\n",
    "      font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "      # Draw the text\n",
    "      #draw.text(position, text, font=font, fill=color)\n",
    "      draw.multiline_text(position, text, font=font, fill=color, align=\"left\")\n",
    "\n",
    "      # Convert the PIL image back to an OpenCV image\n",
    "      image = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "      return image\n",
    "\n",
    "\n",
    "\n",
    "    for result3 in results3.boxes.data.tolist() :\n",
    "        x1, y1, x2, y2, score, class_id = result3\n",
    "\n",
    "\n",
    "\n",
    "        x1+=roi_x1\n",
    "        x2+=roi_x1\n",
    "        y1+=roi_y1\n",
    "        y2+=roi_y1\n",
    "\n",
    "        x1=int(x1)\n",
    "        x2=int(x2)\n",
    "        y1=int(y1)\n",
    "        y2=int(y2)\n",
    "\n",
    "\n",
    "\n",
    "        w=x2-x1\n",
    "        h=y2-y1\n",
    "\n",
    "        ocr_results = []\n",
    "\n",
    "        if score > threshold:\n",
    "\n",
    "            im = frame[y1:y2,x1:x2]\n",
    "            conf = 0.2\n",
    "\n",
    "            gray = cv2.cvtColor(im , cv2.COLOR_RGB2GRAY)\n",
    "            results = reader.readtext(gray)\n",
    "            ocr = \"\"\n",
    "            print(results)\n",
    "\n",
    "            for result in results:\n",
    "                # if len(results) == 1:\n",
    "                #     ocr = result[1]\n",
    "                #     print(ocr)\n",
    "                print(result)\n",
    "\n",
    "                bbox, text, prob = result\n",
    "\n",
    "                # Adjust the bbox coordinates relative to the main frame\n",
    "                bbox = [(int(point[0] + x1), int(point[1] + y1)) for point in bbox]\n",
    "\n",
    "\n",
    "                ocr_results.append(result[1])  # Append the OCR result to the list\n",
    "\n",
    "                #cv2.rectangle(frame, bbox[0], bbox[2], (0,255,0),2)\n",
    "                if len(results) == 1 or len(result[1])>0 and result[2]> conf:\n",
    "                    ocr = result[1]\n",
    "                    #print(ocr)\n",
    "\n",
    "\n",
    "                print(str(ocr))\n",
    "                #bangla_text = str(ocr)\n",
    "                #bangla_text = ocr\n",
    "\n",
    "                bangla_text = \" \".join(ocr_results)\n",
    "                english_text = tr(str(bangla_text), 'ben')\n",
    "                normalized_text = unicodedata.normalize('NFKD', english_text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "                uppercase_text = normalized_text.upper()\n",
    "\n",
    "\n",
    "            # Split the string using regular expression to match whitespace or hyphen as separators\n",
    "            words = re.split(r'\\s|-', uppercase_text)\n",
    "\n",
    "            # Remove any empty strings and leading/trailing whitespace from each word\n",
    "            words = [word.strip() for word in words if word.strip()]\n",
    "            print(words)\n",
    "\n",
    "            target_word = \"\"\n",
    "\n",
    "            for word in words:\n",
    "                if word == \"GO\":\n",
    "                    target_word=\"GA\"\n",
    "                elif word == \"CO\":\n",
    "                    target_word=\"CHA\"\n",
    "\n",
    "                elif word == \"GHO\":\n",
    "                    target_word=\"GHA\"\n",
    "\n",
    "                elif word == \"LO\":\n",
    "                    target_word=\"LA\"\n",
    "\n",
    "                elif word == \"HO\":\n",
    "                    target_word=\"HA\"\n",
    "\n",
    "                elif word == \"CHO\":\n",
    "                    target_word=\"CAA\"\n",
    "                elif word == \"CHO\":\n",
    "                    target_word=\"KA\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(bangla_text)\n",
    "            print(str(english_text))\n",
    "            print(uppercase_text)\n",
    "            #print(number_string)\n",
    "            number_string = extract_number(uppercase_text)\n",
    "            print(f\"Number string: {number_string}\")\n",
    "            print(f\"DHAKA METRO-{target_word} {number_string}\")\n",
    "\n",
    "\n",
    "            position = (10,10)  # (x, y)\n",
    "            # print(position)\n",
    "            font_path = \"/content/drive/MyDrive/YoLo_Thesis/BDTS/Sample Inputs/Siyam Rupali Regular.ttf\"  # Path to a Bangla font file\n",
    "            font_size = 40\n",
    "            color = (0, 0, 255)  # Text color in RGB\n",
    "\n",
    "            # Add the Bangla text to the image\n",
    "\n",
    "            #frame = add_bangla_text(frame, bangla_text, position, font_path, font_size, color)\n",
    "\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 4)\n",
    "            putTextRect(frame, \"Registration Plate\", [int(x1), int(y1) -15],\n",
    "                         scale=1.5, thickness=2,colorR=(255,0,0), offset=7)\n",
    "\n",
    "            putTextRect(frame, f'DHAKA METRO-{target_word} {number_string}', [int(x2)-100, int(y2) + 30],\n",
    "                         scale=1.5, thickness=2,colorR=(0,0,255), offset=7)\n",
    "\n",
    "                 #putTextRect(frame, str(ocr), [int(x1) + 8, int(y2) +30], scale=1.5, thickness=2,colorR=(0,0,255), offset=7)\n",
    "\n",
    "\n",
    "\n",
    "    # detections = sv.Detections.from_ultralytics(results2)\n",
    "    # detections = tracker.update_with_detections(detections)\n",
    "    # bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "    # label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "    # labels = [\n",
    "\n",
    "    #     f\"{tracker_id} {results2.names[class_id].upper()} {(confidence*100):.1f}%\"\n",
    "    #     for class_id, tracker_id, confidence\n",
    "    #     in zip(detections.class_id, detections.tracker_id, detections.confidence)\n",
    "    #   ]\n",
    "\n",
    "    # annotated_image = bounding_box_annotator.annotate(\n",
    "    #       scene=frame, detections=detections)\n",
    "    # annotated_image = label_annotator.annotate(\n",
    "    #       scene=annotated_image, detections=detections, labels=labels)\n",
    "    # annotated_image = trace_annotator.annotate(\n",
    "    #       scene=annotated_image, detections=detections)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    output_path = r\"/content/drive/MyDrive/YoLo_Thesis/BDTS/Sample Inputs/OutputImage.jpg\"\n",
    "    cv2.imwrite(output_path, frame)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
